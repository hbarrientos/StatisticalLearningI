{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc6711e",
   "metadata": {},
   "source": [
    "Universidad Galileo  \n",
    "Maestría en Data Science  \n",
    "Statistical Learning I\n",
    "Sección U  \n",
    "\n",
    "\n",
    "   # Proyecto final: Support Vector Machine (SVM)\n",
    "\n",
    "\n",
    "<p style=\"text-align: right;\">\n",
    "Henry Giovanni<br/>\n",
    "Barrientos García<br/>\n",
    "21001538<br/>\n",
    "Guatemala, 02 de julio de 2022<br/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddffd61",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)\n",
    "Método de clasificación-regresión que fue desarrollado en la década de los 90, dentro de campo de la ciencia computacional. Inicialmente se desarrolló como un método de clasificación binaria, pero su aplicación se ha extendido a problemas de clasificación múltiple y regresión. SVMs ha resultado ser uno de los mejores clasificadores para un amplio abanico de situaciones, por lo que se considera uno de los referentes dentro del ámbito de aprendizaje estadístico y machine learning.\n",
    "\n",
    "**¿Por qué se llaman Máquinas de Vectores de Soporte?**\n",
    "\n",
    "Los vectores de soporte son los puntos que definen el margen máximo de separación del hiperplano que separa las clases. Se llaman vectores, en lugar de puntos, porque estos «puntos» tienen tantos elementos como dimensiones tenga nuestro espacio de entrada. Es decir, estos puntos multi-dimensionales se representan con un vector de n dimensiones.\n",
    "\n",
    "<img src=\"images/SVMMechanism.png\" width=\"75%\" height=\"75%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e919025d",
   "metadata": {},
   "source": [
    "### Hipótesis de SVM\n",
    "\n",
    "El SVM para regresión tiene como meta encontrar un función $f(x)$ que tenga a lo más una desviación de $\\epsilon$ de la salida $y_{i}$, para todos los datos de entrenamiento, y al mismo tiempo, que sea lo más mínima posible.\n",
    "\n",
    "La hipótesis de SVM para regresión es: </br>\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{i=1}^n w_{i}x_{i}+b\n",
    "\\end{equation*}\n",
    "\n",
    "Donde $x_{i}=\\{x_{1},x_{2},...,x_{n}\\}$, $y_{i}=\\{y_{1},y_{2},...,y_{n}\\}$ y $w_{i}=\\{w_{1},w_{2},...,w_{n}\\}$, donde $w$ es el vector de pesos y $b$ es el sesgo.\n",
    "\n",
    "Otra forma de denotar la hipótesis es como se muestra a continuación:\n",
    "\n",
    "<img src=\"images/hipotesis.png\" width=\"50%\" height=\"50%\"/>\n",
    "\n",
    "\n",
    "### Función de Costo\n",
    "\n",
    "En SVM, la función de costo se define como se muestra a continuación:</br>\n",
    "<img src=\"images/loss.png\" width=\"75%\" height=\"75%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ca937f",
   "metadata": {},
   "source": [
    "### Información sobre SVM\n",
    "\n",
    "#### Tipos\n",
    "**Lineal**: Se utiliza para datos que son linealmente separables, es decir, para un conjunto de datos que se puede clasificar en dos categorías utilizando una sola línea recta. Dichos puntos de datos se denominan datos separables linealmente, y el clasificador se utiliza descrito como un clasificador SVM lineal.\n",
    "\n",
    "**No lineal**: Se usa para datos que son datos separables no linealmente, es decir, no se puede usar una línea recta para clasificarlos. Para esto, usamos algo conocido como **truco del kernel** que establece puntos de datos en una dimensión superior donde se pueden separar usando planos u otras funciones matemáticas. Dichos puntos de datos se denominan datos no lineales, y el clasificador utilizado se denomina clasificador SVM no lineal.\n",
    "\n",
    "\n",
    "#### Características\n",
    "\n",
    "- Es ideal y funciona bien con dataset pequeños\n",
    "- Comparado con otros algoritmos, suele tener **overfitting** en menor medida.\n",
    "- Método altamente efectivo\n",
    "- Predicciones no probabilísticas (0 o 1)\n",
    "- Se puede organizar como un problema de optimización convexo, en el que se puede acceder a algoritmos para descubrir el mínimo global de la función objetivo.\n",
    "\n",
    "\n",
    "#### Algunas desventajas\n",
    "\n",
    "- Sensible a anomalías.\n",
    "- Sensible a la escala, por lo cual siempre necesita escalada o normalización.\n",
    "\n",
    "#### Kernel-trick\n",
    "\n",
    "Para modelos no lineales. Kernel son funciones que miden similitudes entre parede de puntos.\n",
    "\n",
    "Hay veces en las que no hay forma de encontrar una hiperplano que permita separar dos clases. En estos casos decimos que las clases no son linealmente separables. Para resolver este problema podemos usar el truco del kernel.\n",
    "\n",
    "El truco del kernel consiste en inventar una dimensión nueva en la que podamos encontrar un hiperplano para separar las clases.\n",
    "\n",
    "<img src=\"https://www.iartificial.net/wp-content/uploads/2019/04/SVM-kernel.png\" width=\"75%\" height=\"75%\"/>\n",
    "\n",
    "#### Kernels comunes\n",
    "\n",
    "<img src=\"images/kernels.png\" width=\"40%\" height=\"40%\"/>\n",
    "\n",
    "#### ¿Cuál kernel utilizar?\n",
    "\n",
    "Lo más recomendado para determinar qué kernel es el más adecuado es crear varios modelos con diferentes kernels, luego estimar para cada uno sus rendimientos y, en última instancia, comparar los resultados. Luego se elige el kernel con los mejores resultados. Se debe ser cuidadoso al estimar el rendimiento del modelo en observaciones diferentes mediante el uso de K-Fold Cross-Validation y considere diferentes métricas como F1 score, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d613f912",
   "metadata": {},
   "source": [
    "### Referencias\n",
    "\n",
    "- https://www.researchgate.net/figure/Support-Vector-Machine-Mechanism_fig1_344827087\n",
    "- https://www.cienciadedatos.net/documentos/34_maquinas_de_vector_soporte_support_vector_machines\n",
    "- https://www.iartificial.net/maquinas-de-vectores-de-soporte-svm/\n",
    "- https://www.upgrad.com/blog/support-vector-machines/\n",
    "- https://biblus.us.es/bibing/proyectos/abreproy/5730/fichero/Resumen+espa%C3%B1ol+PFC+Pablo+Nieto.pdf\n",
    "- https://www.infor.uva.es/~calonso/MUI-TIC/MineriaDatos/SVM.pdf\n",
    "- https://raw.githubusercontent.com/EvanLi/programming-book-3/master/Machine-Learning/Hands%20on%20Machine%20Learning%20with%20Scikit%20Learn%20and%20TensorFlow.pdf\n",
    "- https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-iii-5dff33fa015d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
